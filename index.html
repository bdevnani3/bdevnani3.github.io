<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Bhavika Devnani</title>
  
  <meta name="author" content="Bhavika Devnani">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv='cache-control' content='no-cache'> 
  <meta http-equiv='expires' content='0'> 
  <meta http-equiv='pragma' content='no-cache'>
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Bhavika Devnani</name>
              </p>
              <p>I'm a first year PhD student at <a href="https://faculty.cc.gatech.edu/~judy/">Hoffman AI Research Lab</a> at Georgia Tech. I'm advised by the wonderful Dr. Judy Hoffman.
              </p>
              <p>
                In the past, I've worked on Machine Learning research and engineering at <a href="https://machinelearning.apple.com/">Apple Machine Learning Research</a>, Quora and LinkedIn. 
              </p>
              <p style="text-align:center">
                <a href="mailto:bhavika.devnani@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/BhavikaDevnaniCV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=_ma3b5EAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/bdevnani3/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/BhavikaDevnani.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/BhavikaDevnani.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I've worked on multimodal alignment across a variety of modalities (audio, text, image) for a variety of tasks (retrieval, navigation). 
                I enjoy working with large foundational models. I'm always open to collaborations!
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="elsa_stop()" onmouseover="elsa_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='elsa_image'>
                  <img src='images/contrastive_learning.png' width="160"></div>
                <img src='images/contrastive_learning.png' width="160">
              </div>
              <script type="text/javascript">
                function elsa_start() {
                  document.getElementById('elsa_image').style.opacity = "1";
                }

                function elsa_stop() {
                  document.getElementById('elsa_image').style.opacity = "0";
                }
                elsa_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
	      <a href="https://arxiv.org/abs/2409.11369">	
              <papertitle>ELSA: Learning Spatially-Aware Language and Audio Embeddings </papertitle>
	      </a>
              <br>
              <strong>Bhavika Devnani</strong>, 
              Skyler Seto, 
              Zakaria Aldeneh, 
              Alessandro Toso,
              Yelena Menyaylenko,
              Barry-John Theobald,
              Jonathan Sheaffer, 
              Miguel Sarabia
              <br>
              <em>Under review at NeurIPS</em>, 2024
              <p></p>
              <p>
                Generate dataset and train a model that aligns 3D spatial audio with open vocabulary captions.
              </p>
            </td>
          </tr>	

          
          <tr onmouseout="zson_stop()" onmouseover="zson_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='zson_image'>
                  <img src='images/zson.jpeg' width="160"></div>
                <img src='images/zson.jpeg' width="160">
              </div>
              <script type="text/javascript">
                function zson_start() {
                  document.getElementById('zson_image').style.opacity = "1";
                }

                function zson_stop() {
                  document.getElementById('zson_image').style.opacity = "0";
                }
                zson_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2206.12403">
                <papertitle>ZSON: Zero-Shot Object-Goal Navigation using Multimodal Goal Embeddings</papertitle>
              </a>
              <br>
              Arjun Majumdar*, 
              Gunjan Aggarwal*, 
              <strong>Bhavika Devnani</strong>, 
              Judy Hoffman,
              Dhruv Batra
              <br>
              <em>NeurIPS</em>, 2022
              <p></p>
              <p>
                CLIP enables us to perform Zero-Shot Object-Goal Navigation by learning multimodal goal embeddings.
              </p>
            </td>
          </tr>	

          <tr onmouseout="bisa_stop()" onmouseover="bisa_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='bisa_image'>
                  <img src='images/bisa.jpeg' width="160"></div>
                <img src='images/bisa.jpeg' width="160">
              </div>
              <script type="text/javascript">
                function bisa_start() {
                  document.getElementById('bisa_image').style.opacity = "1";
                }

                function bisa_stop() {
                  document.getElementById('bisa_image').style.opacity = "0";
                }
                bisa_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/BiSA_VTTA_Submission.pdf">
                <papertitle>Bi-Directional Self-Attention for Vision Transformers</papertitle>
              </a>
              <br>
              George Stoica, 
              Taylor Hearn, 
              <strong>Bhavika Devnani</strong>, 
              Judy Hoffman
              <br>
              <em>NeurIPS </em>, Vision Transformers Workshop 2022, <strong>Best Paper</strong>
              <p></p>
              <p>
                Refine a source based on surrounding context by inverting self-attention.
              </p>
            </td>
          </tr>	

        </tbody></table>
					
      </td>
    </tr>
  </table>
</body>

</html>

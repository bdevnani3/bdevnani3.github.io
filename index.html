<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Bhavika Devnani</title>
  
  <meta name="author" content="Bhavika Devnani">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Bhavika Devnani</name>
              </p>
              <p>I'm a Machine Learning Engineer at <a href="https://machinelearning.apple.com/">Apple Machine Learning Research</a>.
              </p>
              <p>
                Before Apple, I did my Master's degree at Georgia Tech, advised by the wonderful <a href="https://www.cc.gatech.edu/~judy/">Professor Judy Hoffman </a> where I worked on problems in Computer Vision and Multimodality. 
              </p>
              <p style="text-align:center">
                <a href="mailto:bhavika.devnani@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/BhavikaDevnaniCV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=_ma3b5EAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/bdevnani3/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/BhavikaDevnani.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/BhavikaDevnani.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                Most of my work so far has been in Computer Vision and Multimodality, specifically in the limited supervision setting.
                Currently, I'm interested in making foundational models (<a href="https://arxiv.org/abs/2103.00020">CLIP</a>, <a href="https://arxiv.org/abs/2102.12092">DALL-E</a>) flexible: finding ways in which we can make them more efficient to train and easily adaptable to downstream tasks.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          
          <tr onmouseout="zson_stop()" onmouseover="zson_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='zson_image'>
                  <img src='images/zson.jpeg' width="160"></div>
                <img src='images/zson.jpeg' width="160">
              </div>
              <script type="text/javascript">
                function zson_start() {
                  document.getElementById('zson_image').style.opacity = "1";
                }

                function zson_stop() {
                  document.getElementById('zson_image').style.opacity = "0";
                }
                zson_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2206.12403">
                <papertitle>ZSON: Zero-Shot Object-Goal Navigation using Multimodal Goal Embeddings</papertitle>
              </a>
              <br>
              Arjun Majumdar*, 
              Gunjan Aggarwal*, 
              <strong>Bhavika Devnani</strong>, 
              Judy Hoffman,
              Dhruv Batra
              <br>
              <em>NeurIPS</em>, 2022
              <p></p>
              <p>
                CLIP enables us to perform Zero-Shot Object-Goal Navigation by learning multimodal goal embeddings.
              </p>
            </td>
          </tr>	

          <tr onmouseout="bisa_stop()" onmouseover="bisa_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='bisa_image'>
                  <img src='images/bisa.jpeg' width="160"></div>
                <img src='images/bisa.jpeg' width="160">
              </div>
              <script type="text/javascript">
                function bisa_start() {
                  document.getElementById('zson_image').style.opacity = "1";
                }

                function bisa_stop() {
                  document.getElementById('zson_image').style.opacity = "0";
                }
                bisa_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/BiSA_VTTA_Submission.pdf">
                <papertitle>Bi-Directional Self-Attention for Vision Transformers</papertitle>
              </a>
              <br>
              George Stoica, 
              Taylor Hearn, 
              <strong>Bhavika Devnani</strong>, 
              Judy Hoffman
              <br>
              <em>NeurIPS </em>, Vision Transformers Workshop 2022, <strong>Best Paper</strong>
              <p></p>
              <p>
                Refine a source based on surrounding context by inverting self-attention.
              </p>
            </td>
          </tr>	

        </tbody></table>
					
      </td>
    </tr>
  </table>
</body>

</html>

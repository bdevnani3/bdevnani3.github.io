<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv='cache-control' content='no-cache'> 
  <meta http-equiv='expires' content='0'> 
  <meta http-equiv='pragma' content='no-cache'>
  <title>Bhavika Devnani</title>
  <meta name="author" content="Bhavika Devnani">
  
  <!-- Google Fonts for modern typography -->
  <link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@400;600&display=swap" rel="stylesheet">

  <!-- Basic styling for layout and color scheme -->
  <style>
    body {
      font-family: 'Open Sans', sans-serif;
      background-color: #f4f4f9;
      color: #333;
      margin: 0;
      padding: 0;
      text-align: center;
    }

    a {
      color: #3498db;
      text-decoration: none;
    }

    a:hover {
      color: #2980b9;
      text-decoration: underline;
    }

    table {
      width: 100%;
      max-width: 800px;
      margin: auto;
      border-spacing: 0;
      border-collapse: separate;
      background-color: #fff;
      box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
      border-radius: 8px;
    }

    td {
      padding: 20px;
    }

    img {
      border-radius: 50%;
      max-width: 100%;
      box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
    }

    h1 {
      font-size: 2.5em;
      font-weight: 600;
      color: #2c3e50;
      margin-top: 0;
    }

    p {
      font-size: 1.1em;
      line-height: 1.6;
      color: #555;
    }

    hr {
      border: none;
      height: 1px;
      background-color: #e2e2e2;
      margin: 40px 0;
    }

    .quote {
      color: #7f8c8d;
      font-style: italic;
    }

    .section-heading {
      font-size: 1.8em;
      color: #2c3e50;
      font-weight: 600;
      margin-bottom: 20px;
    }

    .research-paper {
      margin-top: 40px;
      text-align: left;
    }

    .research-paper strong {
      color: #2c3e50;
    }

    .paper-title {
      font-size: 1.2em;
      color: #34495e;
      font-weight: 600;
    }

    .badge {
      color: #27ae60;
      font-weight: bold;
    }

    /* Hover effect for images */
    .paper img {
      transition: opacity 0.3s ease;
      opacity: 0.8;
    }

    .paper:hover img {
      opacity: 1;
    }
  </style>
</head>
<body>

  <table>
    <tr>
      <td colspan="2">
        <img src="images/BhavikaDevnani.png" alt="Bhavika Devnani" width="150">
        <h1>Bhavika Devnani</h1>
        <p>I'm a first-year PhD student at the <a href="https://faculty.cc.gatech.edu/~judy/">Hoffman AI Research Lab</a> at Georgia Tech, advised by Dr. Judy Hoffman.</p>
        <p>Previously, I worked on machine learning at Apple, Quora, and LinkedIn.</p>
        <p>
          <a href="mailto:bhavika.devnani@gmail.com">Email</a> &nbsp;|&nbsp;
          <a href="data/BhavikaDevnaniCV.pdf">CV</a> &nbsp;|&nbsp;
          <a href="https://scholar.google.com/citations?user=_ma3b5EAAAAJ&hl=en">Google Scholar</a> &nbsp;|&nbsp;
          <a href="https://github.com/bdevnani3/">GitHub</a>
        </p>
        <p class="quote">â€œLearning how to think means being conscious and aware enough to choose how you construct meaning from experience." - DFW</p>
      </td>
    </tr>
    
    <tr>
      <td colspan="2">
        <hr>
        <div class="section-heading">Research</div>
        <p>I'm focused on multimodal alignment (audio, text, image) across tasks like retrieval and navigation. My current work revolves around making foundation models more efficient. I'm always open to collaborations!</p>
      </td>
    </tr>

    <tr class="research-paper">
      <td class="paper" style="width: 25%;">
        <img src='images/contrastive_learning.png' width="160" alt="ELSA Paper">
      </td>
      <td style="width: 75%;">
        <a href="https://arxiv.org/abs/2409.11369" class="paper-title">ELSA: Learning Spatially-Aware Language and Audio Embeddings</a>
        <p><strong>Bhavika Devnani</strong>, Skyler Seto, Zakaria Aldeneh, Alessandro Toso, Yelena Menyaylenko, Barry-John Theobald, Jonathan Sheaffer, Miguel Sarabia</p>
        <p class="badge">Accepted at NeurIPS 2024</p>
        <p>Generated dataset and trained a model to align 3D spatial audio with open vocabulary captions.</p>
      </td>
    </tr>

    <tr class="research-paper">
      <td class="paper" style="width: 25%;">
        <img src='images/zson.jpeg' width="160" alt="ZSON Paper">
      </td>
      <td style="width: 75%;">
        <a href="https://arxiv.org/abs/2206.12403" class="paper-title">ZSON: Zero-Shot Object-Goal Navigation using Multimodal Goal Embeddings</a>
        <p>Arjun Majumdar*, Gunjan Aggarwal*, <strong>Bhavika Devnani</strong>, Judy Hoffman, Dhruv Batra</p>
        <p class="badge">Accepted at NeurIPS 2022</p>
        <p>CLIP enables Zero-Shot Object-Goal Navigation by learning multimodal goal embeddings.</p>
      </td>
    </tr>

    <tr class="research-paper">
      <td class="paper" style="width: 25%;">
        <img src='images/bisa.jpeg' width="160" alt="BiSA Paper">
      </td>
      <td style="width: 75%;">
        <a href="data/BiSA_VTTA_Submission.pdf" class="paper-title">Bi-Directional Self-Attention for Vision Transformers</a>
        <p>George Stoica, Taylor Hearn, <strong>Bhavika Devnani</strong>, Judy Hoffman</p>
        <p class="badge">Accepted at NeurIPS 2022, Vision Transformers Workshop - Best Paper</p>
        <p>Refined sources based on surrounding context by inverting self-attention.</p>
      </td>
    </tr>
  </table>
</body>
</html>
